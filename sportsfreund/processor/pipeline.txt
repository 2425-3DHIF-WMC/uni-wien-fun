Zusammenfassung der KI-basierten Pipeline für den AI-Coach mit Live-Feedback
Das Ziel des AI-Coach-Projekts ist es, live per Webcam Feedback zu Sportübungen zu geben – und zwar so, dass das System nicht für jede Übung explizit programmiert werden muss. Statt Regeln zu schreiben, soll die KI selbst lernen, wann eine Übung ausgeführt wird, wann eine Wiederholung beendet ist und ob Fehler gemacht werden.

1. Datensammlung und Annotation
Die Grundlage bildet ein großer Datensatz aus Videos, die verschiedene Sportübungen zeigen. Diese Videos enthalten mehrere Wiederholungen und sind mit Labels versehen, die angeben, ob eine Wiederholung korrekt oder fehlerhaft ausgeführt wurde – zum Beispiel „nicht tief genug“ oder „zu schnell“. Zusätzlich sind die Videos auf Frame-Ebene annotiert mit Zuständen wie:

Pause (keine Bewegung)

Wiederholung läuft

Wiederholung beendet (Trigger für Feedback)

Diese detaillierte Annotation ermöglicht es der KI, die zeitliche Struktur einer Übung zu verstehen.

2. Feature-Extraktion aus Videos
Um das Modell effizient trainieren zu können, werden aus den Videos Pose-Daten extrahiert, beispielsweise mit Tools wie MediaPipe. Diese Pose-Daten bestehen aus Koordinaten wichtiger Gelenke und Körperpunkte oder daraus abgeleiteten Merkmalen wie Gelenkwinkeln. Die Pose-Daten bilden den Input für das KI-Modell.

3. Trainingsmodell — Sequenz-Labeling mit LSTM oder Transformer
Das Kernstück ist ein zeitlich sensibles neuronales Netz (z. B. LSTM, GRU oder Transformer), das die Pose-Sequenzen verarbeitet. Dieses Modell lernt, für jeden Frame zu bestimmen, ob sich die Person in einer Pause befindet, eine Wiederholung gerade aktiv ausführt oder gerade eine Wiederholung beendet hat.
Dadurch muss keine explizite Logik geschrieben werden, um Wiederholungen zu erkennen oder Pausen zu erkennen – das Modell erlernt diese Muster aus den Trainingsdaten.

4. Feedback-Generierung
Sobald das Modell einen „Wiederholung beendet“-Frame erkennt, kann das System Feedback zur ausgeführten Übung geben. Das Feedback basiert auf den im Training gelabelten Fehlern (z. B. „nicht tief genug“). Das System kann somit live Hinweise geben, wie die Übung verbessert werden kann.

5. Live-Integration auf PC und Raspberry Pi
Zur Laufzeit werden die Live-Webcam-Bilder durch das Pose-Detection-Modul geschickt, das pro Frame die Pose extrahiert. Die Pose-Sequenzen werden dem trainierten Modell als Input gegeben, welches für jeden Frame eine Klassifikation ausgibt. Mit dieser Information steuert die Software, wann Feedback ausgegeben wird (z. B. per Text oder Sprachausgabe).
Um unnötige Berechnungen zu vermeiden und die Performance zu verbessern, wird das Modell so trainiert, dass es auch Pausen erkennt. So gibt es kein ständiges Feedback, sondern nur zu sinnvollen Zeitpunkten.